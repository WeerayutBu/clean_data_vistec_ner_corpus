{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change directory\n",
    "import os\n",
    "# path = '/home/module/'\n",
    "# os.chdir(path)\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dataloader(files, batch_size=1):\n",
    "    for path in files:\n",
    "        with open(path, \"r\") as file:\n",
    "            lines = []\n",
    "            idx   = 0 \n",
    "            \n",
    "            for line in file:\n",
    "                idx += 1\n",
    "                lines.append(line.strip())\n",
    "\n",
    "                # Full batch\n",
    "                if(idx%batch_size==0):\n",
    "                    yield lines\n",
    "                    lines = []\n",
    "\n",
    "            # Not full batch\n",
    "            if(len(lines) != 0): yield lines\n",
    "\n",
    "def get_filename(root_path, pattern='*'):\n",
    "    return [f for f in glob.glob(root_path+pattern, recursive=True)]\n",
    "\n",
    "def save_flatten_data(path_name, sentences):\n",
    "    file   = open(path_name, 'w')\n",
    "    Format = '{}\\n'\n",
    "    for data in sentences:\n",
    "        file.writelines(Format.format(data[0]))\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_nested_data(path_name, sentences):\n",
    "    file   = open(path_name, 'w')\n",
    "    Format = '{}\\n'\n",
    "    for sent in sentences:\n",
    "        for token in sent:\n",
    "            file.writelines(Format.format(token))\n",
    "            \n",
    "        file.writelines(Format.format(''))\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counter_tokens(data):\n",
    "    vocab = Counter()\n",
    "    for idx, batch in enumerate(data):\n",
    "        for sent in batch:\n",
    "            sent = sent.split('|')\n",
    "            vocab.update(sent)            \n",
    "    return vocab\n",
    "\n",
    "def get_counter_nested_tokens(data, tag=False):\n",
    "    vocab = Counter()\n",
    "    for idx, batch in enumerate(data):\n",
    "        for line in batch:\n",
    "            line = line.split('|')\n",
    "            \n",
    "            if(tag == True):\n",
    "                vocab.update(line[1:])  \n",
    "            else:\n",
    "                vocab.update([line[0]])  \n",
    "    return vocab\n",
    "\n",
    "def get_counter_chars(data):\n",
    "    vocab = Counter()\n",
    "    for idx, batch in enumerate(data):\n",
    "        \n",
    "        #Update chars \n",
    "        for sent in batch:\n",
    "            vocab.update(list(sent))\n",
    "    return vocab\n",
    "\n",
    "def Sort_dict_freq(counter_dict, MINCOUNT=1):\n",
    "    Dict  = {(w,c) for w, c in counter_dict.items() if c >= MINCOUNT}\n",
    "    Dict  = sorted(Dict, key=lambda item: item[1], reverse=True)\n",
    "    Dict  = [w[0] for w in Dict]\n",
    "    return Dict\n",
    "\n",
    "def Save_vocab(path_name, Dict, Add_dict=None):\n",
    "    file   = open(path_name, 'w')\n",
    "    Format = '{}\\n'\n",
    "    \n",
    "    # Save add dict\n",
    "    if(Add_dict != None):\n",
    "        for dict_ in Add_dict:\n",
    "            file.writelines(Format.format(dict_))\n",
    "    \n",
    "    # Save dict\n",
    "    for dict_ in Dict:\n",
    "        file.writelines(Format.format(dict_))\n",
    "    file.close\n",
    "    \n",
    "def Save_logs(path_name, Dict):\n",
    "    file   = open(path_name, 'w')\n",
    "    Format = '{}\\n'\n",
    "    \n",
    "    # Save dict\n",
    "    for dict_ in Dict.items():\n",
    "        file.writelines(Format.format(dict_))\n",
    "    file.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_flatten_datas(directory, tag):\n",
    "    print('\\nFlatten     :',tag)\n",
    "    path_data      = directory+'/flatten_ner/'+tag+'/'\n",
    "    save_path_data = directory+'/flatten_ner/'+tag+'/datas/' \n",
    "\n",
    "    data_src       = Dataloader(get_filename(path_data, '*.src'), 1)\n",
    "    data_trg       = Dataloader(get_filename(path_data, '*.trg'), 1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(list(data_src), list(data_trg), test_size=0.2, random_state=42)\n",
    "    X_train, X_val,  y_train, y_val  = train_test_split(X_train, y_train, test_size=0.25, random_state=42)                # 0.25 x 0.8 = 0.2\n",
    "\n",
    "    print('Train datas :', len(X_train))\n",
    "    print('Test  datas :', len(X_test))\n",
    "    print('Valid datas :', len(X_val))\n",
    "    \n",
    "    ## Save dataset ##\n",
    "    # Save train datas\n",
    "    save_flatten_data(save_path_data+'train.src',X_train)\n",
    "    save_flatten_data(save_path_data+'train.trg',y_train)\n",
    "\n",
    "    # Save test datas\n",
    "    save_flatten_data(save_path_data+'test.src',X_test)\n",
    "    save_flatten_data(save_path_data+'test.trg',y_test)\n",
    "\n",
    "    # Save valid datas\n",
    "    save_flatten_data(save_path_data+'valid.src',X_val)\n",
    "    save_flatten_data(save_path_data+'valid.trg',y_val)\n",
    "\n",
    "    ## Create dicts ##\n",
    "    # words\n",
    "    filenames_src  = save_path_data+'words.txt'\n",
    "    counter_dict   = get_counter_tokens(X_train)    #Input data and tokenizer\n",
    "    Dict           = Sort_dict_freq(counter_dict)\n",
    "    Save_vocab(filenames_src, Dict, ['pad','unk'])\n",
    "    Save_logs(filenames_src+'-logs', counter_dict)\n",
    "    print('Words       :', len(Dict)+2)\n",
    "\n",
    "    # Tags\n",
    "    filenames_trg  = save_path_data+'tags.txt'\n",
    "    counter_dict   = get_counter_tokens(y_train)    #Input data and tokenizer\n",
    "    Dict           = Sort_dict_freq(counter_dict)\n",
    "    Save_vocab(filenames_trg, Dict, ['pad'])\n",
    "    Save_logs(filenames_trg+'-logs', counter_dict)\n",
    "    print('Tags        :', len(Dict)+1)\n",
    "\n",
    "    # Save chars dicts\n",
    "    # Chars\n",
    "    # Load text from words dicts\n",
    "    filenames_src  = save_path_data+'words.txt'  \n",
    "    filenames_src  = get_filename(filenames_src)\n",
    "\n",
    "    filenames_chs  = save_path_data+'chars.txt'\n",
    "    data_words     = Dataloader(filenames_src)\n",
    "\n",
    "    counter_dict   = get_counter_chars(data_words)\n",
    "    Dict           = Sort_dict_freq(counter_dict)\n",
    "\n",
    "    # Save\n",
    "    Save_vocab(filenames_chs, Dict, ['pad','unk'])\n",
    "    Save_logs(filenames_chs+'-logs', counter_dict)\n",
    "    print('Chars       :', len(Dict)+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_nested_datas(directory, tag):\n",
    "    print('\\nNested      :',tag)\n",
    "    path_data      = directory+'/nested_ner/'+tag+'/'\n",
    "    save_path_data = directory+'/nested_ner/'+tag+'/datas/' \n",
    "    \n",
    "    # Load dataset\n",
    "    datas = Dataloader(get_filename(path_data, '*.data'))  \n",
    "    \n",
    "    # Get sentences and tokens\n",
    "    sentences   = []\n",
    "    temp_tokens = []\n",
    "    for idx, token in enumerate(datas):\n",
    "        if( token[0] == ''):\n",
    "            sentences.append(temp_tokens)\n",
    "            temp_tokens = []\n",
    "        else:\n",
    "            temp_tokens.append(token[0])\n",
    "\n",
    "    # Train test splits \n",
    "    X_train, X_test = train_test_split(sentences, test_size=0.2,  random_state=42)\n",
    "    X_train, X_val  = train_test_split(X_train,   test_size=0.25, random_state=42)              # 0.25 x 0.8 = 0.2\n",
    "    \n",
    "    print('Train datas :', len(X_train))\n",
    "    print('Test  datas :', len(X_test))\n",
    "    print('Valid datas :', len(X_val))\n",
    "    \n",
    "    save_nested_data(save_path_data+'train.txt',X_train)\n",
    "    save_nested_data(save_path_data+'test.txt',X_test)\n",
    "    save_nested_data(save_path_data+'valid.txt',X_val)\n",
    "    \n",
    "    # Crate words dicts\n",
    "    filenames_src  = save_path_data+'words.txt'\n",
    "    counter_dict   = get_counter_nested_tokens(X_train)    #Input data and tokenizer\n",
    "    Dict           = Sort_dict_freq(counter_dict)\n",
    "    Save_vocab(filenames_src, Dict, ['pad','unk'])\n",
    "    Save_logs(filenames_src+'-logs', counter_dict)\n",
    "    print('Words       :', len(Dict)+2)\n",
    "    \n",
    "    # Create tags dicts\n",
    "    filenames_trg  = save_path_data+'tags.txt'\n",
    "    counter_dict   = get_counter_nested_tokens(X_train, tag=True)    #Input data and tokenizer\n",
    "    Dict           = Sort_dict_freq(counter_dict)\n",
    "    Save_vocab(filenames_trg, Dict, ['pad'])\n",
    "    Save_logs(filenames_trg+'-logs', counter_dict)\n",
    "    print('Tags        :', len(Dict)+1)\n",
    "\n",
    "    # Create chars dicts\n",
    "    filenames_src  = get_filename(save_path_data+'words.txt')\n",
    "    filenames_chs  = save_path_data+'chars.txt'\n",
    "    data_words     = Dataloader(filenames_src)\n",
    "    counter_dict   = get_counter_chars(data_words)\n",
    "    Dict           = Sort_dict_freq(counter_dict)\n",
    "\n",
    "    Save_vocab(filenames_chs, Dict, ['pad','unk'])\n",
    "    Save_logs(filenames_chs+'-logs', counter_dict)\n",
    "    print('Chars       :', len(Dict)+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Flatten     : maintags\n",
      "Train datas : 1719\n",
      "Test  datas : 573\n",
      "Valid datas : 573\n",
      "Words       : 21912\n",
      "Tags        : 42\n",
      "Chars       : 206\n",
      "\n",
      "Flatten     : subtags\n",
      "Train datas : 1719\n",
      "Test  datas : 573\n",
      "Valid datas : 573\n",
      "Words       : 21912\n",
      "Tags        : 389\n",
      "Chars       : 206\n",
      "\n",
      "Nested      : maintags\n",
      "Train datas : 1719\n",
      "Test  datas : 573\n",
      "Valid datas : 573\n",
      "Words       : 21912\n",
      "Tags        : 42\n",
      "Chars       : 206\n",
      "\n",
      "Nested      : subtags\n",
      "Train datas : 1719\n",
      "Test  datas : 573\n",
      "Valid datas : 573\n",
      "Words       : 21912\n",
      "Tags        : 399\n",
      "Chars       : 206\n"
     ]
    }
   ],
   "source": [
    "state          = 'split_flatten_datas'\n",
    "directory      = '/home/module/data/vistec_newmm4L'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if(state=='split_flatten_datas'): \n",
    "        split_flatten_datas(directory,'maintags')\n",
    "        split_flatten_datas(directory,'subtags')\n",
    "        \n",
    "        split_nested_datas(directory,'maintags')\n",
    "        split_nested_datas(directory,'subtags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' : newmm\n",
    "# Flatten     : maintags\n",
    "# Train datas : 1200\n",
    "# Test  datas : 400\n",
    "# Valid datas : 400\n",
    "# Words       : 18454\n",
    "# Tags        : 387\n",
    "# Chars       : 206\n",
    "\n",
    "# Flatten     : subtags\n",
    "# Train datas : 1200\n",
    "# Test  datas : 400\n",
    "# Valid datas : 400\n",
    "# Words       : 18454\n",
    "# Tags        : 42\n",
    "# Chars       : 206\n",
    "\n",
    "# Nested      : maintags\n",
    "# Train datas : 1200\n",
    "# Test  datas : 400\n",
    "# Valid datas : 400\n",
    "# Words       : 18454\n",
    "# Tags        : 400\n",
    "# Chars       : 206\n",
    "\n",
    "# Nested      : subtags\n",
    "# Train datas : 1200\n",
    "# Test  datas : 400\n",
    "# Valid datas : 400\n",
    "# Words       : 18454\n",
    "# Tags        : 42\n",
    "# Chars       : 206\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Attacut\n",
    "# Flatten     : maintags\n",
    "# Train datas : 1200\n",
    "# Test  datas : 400\n",
    "# Valid datas : 400\n",
    "# Words       : 17459\n",
    "# Tags        : 387\n",
    "# Chars       : 206\n",
    "\n",
    "# Flatten     : subtags\n",
    "# Train datas : 1200\n",
    "# Test  datas : 400\n",
    "# Valid datas : 400\n",
    "# Words       : 17459\n",
    "# Tags        : 42\n",
    "# Chars       : 206\n",
    "\n",
    "# Nested      : maintags\n",
    "# Train datas : 1200\n",
    "# Test  datas : 400\n",
    "# Valid datas : 400\n",
    "# Words       : 17459\n",
    "# Tags        : 402\n",
    "# Chars       : 206\n",
    "\n",
    "# Nested      : subtags\n",
    "# Train datas : 1200\n",
    "# Test  datas : 400\n",
    "# Valid datas : 400\n",
    "# Words       : 17459\n",
    "# Tags        : 42\n",
    "# Chars       : 206\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attacut\n",
    "# Flatten     : maintags\n",
    "# Train datas : 1077\n",
    "# Test  datas : 360\n",
    "# Valid datas : 359\n",
    "# Words       : 16103\n",
    "# Tags        : 42\n",
    "# Chars       : 184\n",
    "\n",
    "# Flatten     : subtags\n",
    "# Train datas : 1077\n",
    "# Test  datas : 360\n",
    "# Valid datas : 359\n",
    "# Words       : 16103\n",
    "# Tags        : 374\n",
    "# Chars       : 184\n",
    "\n",
    "# Nested      : maintags\n",
    "# Train datas : 1077\n",
    "# Test  datas : 360\n",
    "# Valid datas : 359\n",
    "# Words       : 16103\n",
    "# Tags        : 42\n",
    "# Chars       : 184\n",
    "\n",
    "# Nested      : subtags\n",
    "# Train datas : 1077\n",
    "# Test  datas : 360\n",
    "# Valid datas : 359\n",
    "# Words       : 16103\n",
    "# Tags        : 388\n",
    "# Chars       : 184"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newmm\n",
    "\n",
    "# Flatten     : maintags\n",
    "# Train datas : 1077\n",
    "# Test  datas : 360\n",
    "# Valid datas : 359\n",
    "# Words       : 17348\n",
    "# Tags        : 42\n",
    "# Chars       : 184\n",
    "\n",
    "# Flatten     : subtags\n",
    "# Train datas : 1077\n",
    "# Test  datas : 360\n",
    "# Valid datas : 359\n",
    "# Words       : 17348\n",
    "# Tags        : 373\n",
    "# Chars       : 184\n",
    "\n",
    "# Nested      : maintags\n",
    "# Train datas : 1077\n",
    "# Test  datas : 360\n",
    "# Valid datas : 359\n",
    "# Words       : 17348\n",
    "# Tags        : 42\n",
    "# Chars       : 184\n",
    "\n",
    "# Nested      : subtags\n",
    "# Train datas : 1077\n",
    "# Test  datas : 360\n",
    "# Valid datas : 359\n",
    "# Words       : 17348\n",
    "# Tags        : 385\n",
    "# Chars       : 184\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:model] *",
   "language": "python",
   "name": "conda-env-model-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
