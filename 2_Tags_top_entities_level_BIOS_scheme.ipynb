{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file\n",
    "data_csv  = pd.read_csv('./../pct_wn_first_lot.csv')\n",
    "read_file = open(\"./../ner_lot1_110520_num_word_192449_num_tag_20702.json\", \"r\")\n",
    "data_json = json.load(read_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize entities and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attacut import tokenize, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_entities2word(set_index, txt_test_data):\n",
    "    # Dumps and load json data\n",
    "    txt_test_data      = json.dumps(txt_test_data)\n",
    "    txt_test_data      = json.loads(txt_test_data)\n",
    "    temp               = 0\n",
    "    save_token         = []\n",
    "    \n",
    "    for idx, _ in enumerate(range(len(txt_test_data))):\n",
    "        \n",
    "        # Tokenize entities\n",
    "        if idx in set_index:\n",
    "            e_token = txt_test_data[temp:idx]\n",
    "\n",
    "            #Check empty text\n",
    "            if(len(e_token) > 0):\n",
    "                \n",
    "                # Tokenize each sentence\n",
    "                words_tokenize = tokenize(e_token)\n",
    "                save_token.extend(words_tokenize)\n",
    "                temp = idx\n",
    "    save_token.extend(tokenize(txt_test_data[temp:]))\n",
    "    return save_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tags top entities level by BIOS scheme "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_index(idx, entities_list):\n",
    "    tags  = []\n",
    "    for idx_ne, entity in enumerate(entities_list):\n",
    "        en_type, en_idx = entity\n",
    "        \n",
    "        # Return first entity\n",
    "        if(idx in range(en_idx[0],en_idx[1])):\n",
    "            return (idx_ne,'', en_type)\n",
    "        \n",
    "    return (-1,'O', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_BIOS_tokenzie(word, tag, file):\n",
    "    save_format   = '{} {}\\n'.format(word, tag[1]+tag[2])\n",
    "    file.write(str(save_format))\n",
    "    \n",
    "def tags_BIOS_top_entities_level(word_token, en_boundary, file):\n",
    "    count_top_en  = 0\n",
    "    words_tags    = []\n",
    "    shift_index   = 0\n",
    "    BIOS_tags     = ''\n",
    "    \n",
    "    # Initial value NE1 NE2\n",
    "    NE1           = get_entity_index(-1, en_boundary)\n",
    "    NE2           = get_entity_index(shift_index, en_boundary)\n",
    "    \n",
    "    if((NE1[1] in ['S', 'E'] or NE1[0] == -1) and NE2[0] != -1): BIOS_tags = 'S'\n",
    "    NE2           = (NE2[0], BIOS_tags, NE2[2])\n",
    "\n",
    "\n",
    "    for word in word_token:\n",
    "        BIOS_tags   = ''\n",
    "        shift_index += len(word)\n",
    "        \n",
    "        # Get tag NE3\n",
    "        NE3         = get_entity_index(shift_index, en_boundary)\n",
    "        NE3         = (NE3[0], BIOS_tags, NE3[2])\n",
    "\n",
    "        # Initial value\n",
    "        if( NE3[0] != -1 ):\n",
    "            #'SEO->SOB'\n",
    "            if(NE2[1] in ['S', 'E'] or NE2[0] == -1):\n",
    "                BIOS_tags = 'S'\n",
    "\n",
    "            #'BI->IE'\n",
    "            elif(NE2[1] in ['B', 'I']): \n",
    "                BIOS_tags = 'I'\n",
    "        else:\n",
    "            BIOS_tags = 'O'\n",
    "\n",
    "        NE3 = (NE3[0], BIOS_tags, NE3[2])\n",
    "\n",
    "        # Update NE2 and NE3\n",
    "        if(NE2[0] == NE3[0]):\n",
    "            if(NE2[1] == 'S'):\n",
    "                NE2   = (NE2[0], 'B', NE2[2])\n",
    "                NE3   = (NE3[0], 'I', NE3[2])\n",
    "\n",
    "        elif(NE2[1] == 'I'):\n",
    "                NE2   = (NE2[0], 'E', NE2[2])\n",
    "\n",
    "        # Shift NE2 to NE1\n",
    "        if(NE2[0] != -1):\n",
    "            NE1 = (NE2[0], NE2[1]+'-', NE2[2])\n",
    "        else :\n",
    "            NE1 = (NE2[0], NE2[1], NE2[2])\n",
    "\n",
    "        # Shift NE3 to NE2\n",
    "        NE2 = NE3\n",
    "        \n",
    "        # Save BIOS tokenize\n",
    "        save_BIOS_tokenzie(word, NE1, file)\n",
    "        \n",
    "        # Count top-level tokens\n",
    "        if(NE1[0] != -1):\n",
    "            if(NE1[1] in ['S-', 'E-']): \n",
    "                count_top_en+=1\n",
    "        \n",
    "        \n",
    "    # Add stop sentence and close file\n",
    "    file.write('\\n')\n",
    "    \n",
    "    return count_top_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tags and save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Results : 800 sentences, 209326 words, 10860 top-level tokens\n"
     ]
    }
   ],
   "source": [
    "# Load data \n",
    "tags_ne_json      = data_json\n",
    "segment_text      = data_csv['ssg']\n",
    "clean_text        = data_csv['text_clean']\n",
    "\n",
    "# Remove pipe\n",
    "clean_data        = remove_pipe(tags_ne_json, segment_text, clean_text)\n",
    "\n",
    "# Open file for save data\n",
    "file = open('example_data_word_BIOS_Tagging.txt', 'w')\n",
    "\n",
    "# Count word and sentence\n",
    "word_count   = 0\n",
    "sent_count   = 0\n",
    "top_en_count = 0\n",
    "\n",
    "for idx, data in enumerate(clean_data):\n",
    "    \n",
    "    # Prepare data\n",
    "    entities            = data['entities']\n",
    "    text                = data['text']\n",
    "    set_index_boundary  = get_all_entities_index(entities)\n",
    "    en_boundary         = get_entities_boundary(entities)\n",
    "\n",
    "    # Tokenize word\n",
    "    words_token         = tokenize_entities2word(set_index_boundary, text)\n",
    "    # Count word\n",
    "    word_count          += len(words_token)\n",
    "    sent_count          += 1\n",
    "    top_en_count        += tags_BIOS_top_entities_level(words_token, en_boundary, file)\n",
    "    \n",
    "# Close file\n",
    "file.close\n",
    "    \n",
    "print('\\n\\nResults : {} sentences, {} words, {} top-level tokens'.format(sent_count, word_count, top_en_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check entities from raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sukishi  สา ขา Mega Bangna  ตัว ร้าน จะ อยู่ โซน  food walk  ค่ะ จะ อยู่ ใกล้ ๆ กับ  star buck  เลย  ครั้ง นี้ มา ตรง กับ วัน หยุด  อืัอ หือ ออ คน อย่าง แน่น เลย ย  ต้อง รอ คิว ค่ะ  วัน นี้ รอ3 คิว  ก็ รอ ไม่ นาน แปป เดียว  ตัว ร้าน ที่ สา ขา นี้ ถือ ว่า กว้าง มาก ๆ เลย ค่ะ  จัด ร้าน ดู โปร่ง โล่ง สบาย ด้วย  ช่วง นี้ มี เฮ    เพราะ เค้า ลด รา คา สำ หรับ อา หาร หลาย ๆ ชุด ค่ะ  ก็ จัด มา1 ชุด  จำ รา คา ไม่ ได้ ไม่ แน่ ใจ ว่า  799.  หรือ เปล่า  อ้ะ ะะ หิว มาก ก็ จัด ซะ เลย ย  เนื้อ นุ่ม มาก อร่อย ด้วย คือ ดี คือ ฟิน นน น  ยิ่ง เป็น เบ คอน ยิ่ง อร่อย เลย ค่ะ  รส ชาติ หมู และ ความ นุ่ม ยก ให้ สุ กี้ ชิ เลย  เครื่อง ดื่ม  สั่ง ชา มะ นาว และ โก โก้ เฟรป เป้ มา  โก โก้ รส ชาติ ใช้ ได้ เลย ค่ะ  แต่ หวาน มาก ไป นิด นึง  นึก ถึง ปิ้ง ย่าง นึก ถึง  Sukishi  เลย ค่ะ ยก ใจ ให้ เลย \n",
      "\n",
      "    0 : 7     restaurant           Sukishi\n",
      "   15 : 26    facility:other       Mega Bangna\n",
      "   15 : 19    loc:others           Mega\n",
      "   20 : 26    district             Bangna\n",
      "   50 : 59    loc:others           food walk\n",
      "   50 : 54    loc:others           food\n",
      "   85 : 94    restaurant           star buck\n",
      "  192 : 193   cardinal             3\n",
      "  194 : 197   unit                 คิว\n",
      "  388 : 389   cardinal             1\n",
      "  390 : 393   unit                 ชุด\n",
      "  428 : 431   cardinal             799\n",
      "  535 : 541   food:ingredient      เบ คอน\n",
      "  595 : 604   restaurant           สุ กี้ ชิ\n",
      "  595 : 601   food:ingredient      สุ กี้\n",
      "  747 : 754   restaurant           Sukishi\n"
     ]
    }
   ],
   "source": [
    "sentence_no  = 0\n",
    "tags_ne_json = sorted(data_json[sentence_no]['entities'], key=lambda x:(x['start_idx'], -x['end_idx']))\n",
    "print(data_json[sentence_no]['text'],'\\n')\n",
    "[print('{:>5} : {:<5} {:20} {}'.format(x['start_idx'], x['end_idx'], x['type'], x['text'])) for x in tags_ne_json]\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show example data after remove pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "\tSukishi สาขา Mega Bangna ตัวร้านจะอยู่โซน food walk ค่ะจะอยู่ใกล้ๆกับ star buck เลย ครั้งนี้มาตรงกับวันหยุด อืัอหือออคนอย่างแน่นเลยย ต้องรอคิวค่ะ วันนี้รอ3คิว ก็รอไม่นานแปปเดียว ตัวร้านที่สาขานี้ถือว่ากว้างมากๆเลยค่ะ จัดร้านดูโปร่งโล่งสบายด้วย ช่วงนี้มีเฮ  เพราะเค้าลดราคาสำหรับอาหารหลายๆชุดค่ะ ก็จัดมา1ชุด จำราคาไม่ได้ไม่แน่ใจว่า 799. หรือเปล่า อ้ะะะหิวมากก็จัดซะเลยย เนื้อนุ่มมากอร่อยด้วยคือดีคือฟินนนน ยิ่งเป็นเบคอนยิ่งอร่อยเลยค่ะ รสชาติหมูและความนุ่มยกให้สุกี้ชิเลย เครื่องดื่ม สั่งชามะนาวและโกโก้เฟรปเป้มา โกโก้รสชาติใช้ได้เลยค่ะ แต่หวานมากไปนิดนึง นึกถึงปิ้งย่างนึกถึง Sukishi เลยค่ะยกใจให้เลย\n",
      "\n",
      "\n",
      "text                          \t\ttype                 start_idx <-> end_idx\n",
      "            \n",
      "Sukishi                        \t\trestaurant               0 <-> 7                    --Sukishi--\n",
      "Mega Bangna                    \t\tfacility:other          13 <-> 24                   --Mega Bangna--\n",
      "Mega                           \t\tloc:others              13 <-> 17                   --Mega--\n",
      "Bangna                         \t\tdistrict                18 <-> 24                   --Bangna--\n",
      "food walk                      \t\tloc:others              42 <-> 51                   --food walk--\n",
      "food                           \t\tloc:others              42 <-> 46                   --food--\n",
      "star buck                      \t\trestaurant              70 <-> 79                   --star buck--\n",
      "3                              \t\tcardinal               154 <-> 155                  --3--\n",
      "คิว                            \t\tunit                   155 <-> 158                  --คิว--\n",
      "1                              \t\tcardinal               302 <-> 303                  --1--\n",
      "ชุด                            \t\tunit                   303 <-> 306                  --ชุด--\n",
      "799                            \t\tcardinal               331 <-> 334                  --799--\n",
      "เบ คอน                         \t\tfood:ingredient        413 <-> 418                  --เบคอน--\n",
      "สุ กี้ ชิ                      \t\trestaurant             459 <-> 466                  --สุกี้ชิ--\n",
      "สุ กี้                         \t\tfood:ingredient        459 <-> 464                  --สุกี้--\n",
      "Sukishi                        \t\trestaurant             575 <-> 582                  --Sukishi--\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show example data after remove pipe\n",
    "sentence_no  = 0\n",
    "check_sentence(clean_data[sentence_no])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sukishi S-restaurant\n",
      "  O\n",
      "สาขา O\n",
      "  O\n",
      "Mega B-facility:other\n",
      "  I-facility:other\n",
      "Bangna E-facility:other\n",
      "  O\n",
      "ตัว O\n",
      "ร้าน O\n"
     ]
    }
   ],
   "source": [
    "! head example_data_word_BIOS_Tagging.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:model]",
   "language": "python",
   "name": "conda-env-model-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
